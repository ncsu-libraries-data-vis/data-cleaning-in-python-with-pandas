{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Cleaning_in_Python_with_pandas.ipynb","provenance":[{"file_id":"1mKI4HMPHHP_UXk3ys3CnjmEE9QS7Mk2H","timestamp":1619716981109},{"file_id":"12Rme5FLxDFpTFAldfgsleGG6eyXw2ewl","timestamp":1615288363335}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FpbBQzdwReOX"},"source":["# Data cleaning in Python with pandas\n","\n","##  Setup\n","\n","With this Google Colaboratory (Colab) notebook open, click the \"Copy to Drive\" button that appears in the menu bar. The notebook will then be attached to your own Google user account, so you can edit it in any way you like -- you can even take notes directly in the notebook.\n","\n","## Agenda\n","\n","1. **Guided instruction**: An instructor will work through the guided activity and teach concepts along the way.\n","\n","2. **Collaborative exercises**: After the guided portion of the workshop, we will have time to work collaboratively on provided exercises, ask questions, and discuss any concepts relevant to the materials. (Exercises can also be completed outside of the meeting time. A link to the filled version of this notebook containing all of the completed code is available at the end of the notebook under the section *Further resources* for reference)\n","\n","## Learning objectives\n","\n","By the end of our workshop today, we hope you'll understand what the pandas library is and be able to use pandas to load, explore, and manipulate data.\n","\n","### Today's Topics\n","\n","- Reading and writing data files\n","\n","- Exploring dataset characteristics\n","\n","- Manipulating a dataset (adding/removing/editing data)\n","\n","- Filtering a dataset"]},{"cell_type":"markdown","metadata":{"id":"YohJ7IZeTX31"},"source":["### Using Jupyter Notebooks and Google Colaboratory\n","\n","Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop.  Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n","\n","Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we're happy to help. Feel free to [get help from our graduate consultants](https://www.lib.ncsu.edu/dxl) or [schedule an appointment with Libraries staff](https://go.ncsu.edu/dvs-request)."]},{"cell_type":"markdown","metadata":{"id":"diWLAlRBTsWT"},"source":["## Guided Instruction\n","\n","**Content Warning:** This dataset contains information relating to violence towards animals. We understand that this may be distressing, and if you need to step away from the workshop we understand.\n","\n","In this section, we will work through examples using data from the [Federal Aviation Administration (FAA) Wildlife Strikes Database](https://wildlife.faa.gov/search). We have filtered the data to only include North Carolina.\n","\n","> \"The FAA Wildlife Strike Database contains records of reported wildlife strikes since 1990. Strike reporting is voluntary. Therefore, this database only represents the information we have received from airlines, airports, pilots, and other sources.\" - [FAA website](https://wildlife.faa.gov/home)"]},{"cell_type":"markdown","metadata":{"id":"0RmzdMW1dPzA"},"source":["### Import the data and the pandas library\n","\n","To start, we must first load the pandas library into our Python environment and load in our datasets"]},{"cell_type":"markdown","metadata":{"id":"XVC9nCCOWqqA"},"source":["#### Import pandas\n","\n","Pandas is a high-level data manipulation tool first created in 2008 by Wes McKinney. The name comes from the term “panel data,” an econometrics term for data sets that include observations over multiple time periods for the same individuals.<sup>[[wikipedia](https://en.wikipedia.org/wiki/Pandas_(software))]</sup>\n","\n","From Jake Vanderplas’ book [*Python Data Science Handbook*](http://shop.oreilly.com/product/0636920034919.do):\n","\n","> As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs.\n","\n","#### What does Pandas do?\n","\n","- Reading and writing data from persistent storage\n","\n","- Cleaning, filtering, and otherwise preparing data\n","\n","- Calculating statistics and analyzing data\n","\n","- Visualization with help from the Matplotlib plotting library"]},{"cell_type":"code","metadata":{"id":"hsS2thybwJpY"},"source":["# Import the Pandas library as pd (callable in our code as pd)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77IUEVvoNhpJ"},"source":["#### Import datasets\n","\n","We have prepared the data from the FAA website for this workshop. We will import those datasets into our notebook to use them for this activity.\n","\n","- [Preview the CSV file (opens on GitHub)](https://github.com/NCSU-Libraries/data-viz-workshops/blob/master/Python_Open_Labs/data/FAA_Wildlife_strikes_1990-1999.csv) - wildlife strike data from the years 1990-1999\n","-[Preview the Excel file (this link will download the file)](https://github.com/NCSU-Libraries/data-viz-workshops/blob/master/Python_Open_Labs/data/FAA_Wildlife_strikes_2000-2009.xlsx?raw=true) - wildlife strike data from the years 2000-2009\n","- [Preview the JSON file (opens on GitHub)](https://raw.githubusercontent.com/NCSU-Libraries/data-viz-workshops/master/Python_Open_Labs/data/FAA_Wildlife_strikes_2010-2019.json) - wildlife strike data from the years 2010-2019"]},{"cell_type":"code","metadata":{"id":"pfCttfhqNhpS"},"source":["# Import the CSV file (wildlife strike data from the years 1990-1999)\n","csv_file_url = 'https://raw.githubusercontent.com/ncsu-libraries-data-vis/data-cleaning-in-python-with-pandas/main/data/FAA_Wildlife_strikes_1990-1999.csv'\n","\n","\n","# Print out the first five columns of the dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKTp-O2vNhpT"},"source":["# Import the Excel file (wildlife strike data from the years 2000-2009)\n","xls_file_url = 'https://github.com/ncsu-libraries-data-vis/data-cleaning-in-python-with-pandas/blob/main/data/FAA_Wildlife_strikes_2000-2009.xlsx?raw=true'\n","\n","\n","# Print out the first five columns of the dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DyV7h5pNhpU"},"source":["#Import the JSON file (wildlife strike data from the years 2010-2019)\n","json_file_url = 'https://raw.githubusercontent.com/ncsu-libraries-data-vis/data-cleaning-in-python-with-pandas/main/data/FAA_Wildlife_strikes_2010-2019.json'\n","\n","\n","# Print out the first five columns of the dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rij07LDXEPg"},"source":["### Explore the data\n","\n","Now that we have our data, we can use pandas to explore it. This can be useful if you are new to a dataset and want to understand what it contains.\n","\n","#### Pandas data structures\n","\n","Pandas uses two main data structures: `Series` and `DataFrame`. **The data we imported is structured as a `DataFrame`.**\n","\n","<img src=\"https://raw.githubusercontent.com/NCSU-Libraries/data-viz-workshops/master/Data_Manipulation_with_Python/assets/nc_dataframes.png\" alt=\"DataFrames are composed of Series\" width=\"60%\">\n","\n","A `Series` is a one-dimensional array of indexed data, or a single column of data with labels. You can learn more about the Series data type in the [Pandas documentation for Series](https://pandas.pydata.org/pandas-docs/stable/reference/series.html).\n","\n","A `DataFrame` is a two-dimensional array composed of two or more `Series`, similar to tabluar data (think of Excel). They have row labels (an `Index`) and column labels. You can learn more about the DataFrame data type in the [Pandas documentation for DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html).\n","\n","A `DataFrame` is made up of `Series` in a similar way in which a table is made up of columns. The only restriction is that each column must be of the same data type. Many of the operations that can be performed on a `DataFrame` can also be performed on an individual `Series`."]},{"cell_type":"markdown","metadata":{"id":"QYn7Bu8rHv6i"},"source":["#### Referencing and indexing a DataFrame\n","\n","The pandas library provides utilities to easily reference specific columns and rows from our DataFrame. "]},{"cell_type":"markdown","metadata":{"id":"Zesk0tcPe4wB"},"source":["##### Referencing columns"]},{"cell_type":"code","metadata":{"id":"IGHafxVsHaLG"},"source":["# View column labels (headers)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utCgHZj9cE_6"},"source":["# Reference a column from a DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_8cDwmJc1Ol"},"source":["# Reference multiple columns from a DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tt2aTlyCfAAP"},"source":["##### Referencing rows"]},{"cell_type":"code","metadata":{"id":"ASqEGDSjbtnn"},"source":["# View the row index labels from a DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0eZWXdgcaDB"},"source":["# Reference a row by index label using .loc\n","\n","# Access first row of wl_strikes_csv by index label\n","# In this case the index label is 0\n","\n","\n","# Access first row of wl_strikes_json by index label\n","# In this case the index label is not 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDv5Wqk-faL9"},"source":["# Reference multiple rows by index label (in this case the index label 0 through 2)\n","# Returns a DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLB_TnBLfgHy"},"source":["# Reference a row or multiple rows by zero-based integer position\n","\n","# Access first row of wl_strikes_csv by row integer value\n","# In this case the row is row 0\n","\n","\n","# Access first row of wl_strikes_json by row integer value\n","# In this case the row is also row 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4XI12irflm2"},"source":["# Reference multiple rows by row number (in this case rows 0 through 2)\n","# Note that this time the range doesn't include the stop number\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhNlJ2h7fn5z"},"source":["##### Referenceing both rows and columns"]},{"cell_type":"code","metadata":{"id":"_8cd1b1wfspk"},"source":["# Referencing a subset of rows and columns using index and column labels\n","# Note that we're using a range of column labels instead of a list\n","# Make sure that your column range starts with the leftmost label\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DrgiOCTevH3P"},"source":["#### View summaries of a DataFrame\n","\n","We can quickly generate summaries of our DataFrame to observe some basic statistics and information such as column data types and non-null value counts."]},{"cell_type":"code","metadata":{"id":"vlIAaeihhVg2"},"source":["# Get the \"shape\" of the Dataframe (the number of rows and columns)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RB9nwV--sd5a"},"source":["# Get summary statistics of DataFrame columns using \"describe()\" (only includes\n","# numerical data types)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOCCBvV043UT"},"source":["# Get summary statistics of single column using \"describe()\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dI5VjlC71rY8"},"source":["# Summarize column data types, non-null values, and memory usage using \"info()\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmFKadP-Y7OL"},"source":["### Manipulate the data\n","\n","After exploring our data, we can use pandas to add, remove, and change data to create a dataset or multiple datasets that are more suited for our analyses."]},{"cell_type":"markdown","metadata":{"id":"hZ6_a__au-7z"},"source":["#### Reset DataFrame index labels\n","The JSON file we imported does not include the column `INDX_NR`. Instead, these values are used as the index labels. We want this dataset to match the format of our other datasets, so we first need to reset the index using the DataFrame method `reset_index()`."]},{"cell_type":"code","metadata":{"id":"NexfOR1xPRcF"},"source":["# Print out the first five rows of the JSON DataFrame\n","\n","\n","# Reset the JSON DataFrame index\n","\n","\n","# Print out the first five rows of the dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rigiVVn3PpB"},"source":["#### Rename column labels\n","\n","When we reset our index a new column `index` was created. Let's change the name of this column to `INDX_NR` to match our other datasets using the DataFrame `rename()` method."]},{"cell_type":"code","metadata":{"id":"k0AUqkC2OD5v"},"source":["# Rename the column we created\n","\n","\n","# Print out the first five rows of the dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBKmROb2YK6_"},"source":["#### Concatenate the three DataFrames\n","\n","We want to be able to work with all of the data we have imported at once, so we need to pull all three DataFrames into one. They all have the same columns now, so we can concatenate them based on columns (similar to adding them together, one on top of another) using the pandas method `concat()`. We also need to consider the current index labels for each dataset. We will create a new zero-based integer index label for the concatenated dataset by passing the keyword argument `ignore_index=True` into the `concat()` method."]},{"cell_type":"code","metadata":{"id":"sEIc3jUzYNq5"},"source":["# Concatenate all the datasets into one\n","\n","\n","# Print the shape (number of rows and columns) of the full DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HnEJzysJ4N2h"},"source":["#### Merge DataFrames\n","\n","Our dataset includes a column of species IDs (`SPECIES_ID`) that consist of alpha-numeric codes that reference a specific species of animal. This code is not very helpful if we want to know the species name of an animal involved in a strike. Let's join our dataset with another dataset containing unique species IDs and species names using the shared column `SPECIES_ID` to generate a new column of data (`SPECIES`) containing species name using `merge()`. The URL to the dataset of species IDs and names is stored in the variable `species_names_file_url`."]},{"cell_type":"code","metadata":{"id":"gTL942qP7-so"},"source":["# Load the species ID table (stored in a CSV file)\n","species_names_file_url = 'https://raw.githubusercontent.com/ncsu-libraries-data-vis/data-cleaning-in-python-with-pandas/main/data/FAA_Wildlife_species_id_table.csv'\n","\n","\n","# Print the loaded species ID table\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLUpy4CaexGD"},"source":["![Left join visual example](https://github.com/NCSU-Libraries/data-viz-workshops/blob/master/Python_Open_Labs/Data_wrangling_with_Pandas/left-join.png?raw=true)"]},{"cell_type":"code","metadata":{"id":"G0iqjbAUm9_9"},"source":["# Create a new DataFrame from a \"left\" join of the full dataset and the species\n","# ID table based on the shared column \"SPECIES_ID\"\n","\n","\n","# Print out the columns \"SPECIES\" and \"SPECIES_ID\" from the new merged dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GcwpUDzN34rj"},"source":["#### Removing unnecessary columns\n","\n","We can reduce the size of our dataset by removing unnecessary columns of data using the DataFrame `drop()` method."]},{"cell_type":"code","metadata":{"id":"z1YZ_4pE7ZBU"},"source":["# Remove the \"STATE\", \"FAAREGION\", and \"COMMENT\" columns using \"drop()\"\"\n","\n","\n","# Print out the shape of the new DataFrame (should only contain 89 columns)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLMysVq9yKYX"},"source":["#### Calculating new columns\n","\n","We can add new data to our dataset by manipulating existing data."]},{"cell_type":"markdown","metadata":{"id":"ZEtlT-TaHjfA"},"source":["##### Create a new column using an expression\n","\n","We may want to add a new column that is calculated based on other columns. In this example, we create a new column (`SINGLE_OR_MULTI_ENGINE`) of boolean values that tells us if the plane was a single-engine (TRUE) or a multi-engine (FALSE) plane using a comparison operator to test if the value in the column `NUM_ENGS` equals 1."]},{"cell_type":"code","metadata":{"id":"IL3zoS0cykQ8"},"source":["# Print out the \"NUM_ENGS\" column\n","\n","\n","# Create a new column of boolean values indicating single- or multi-engine\n","\n","\n","# Print out the new column\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0etw8hbjI6QZ"},"source":["##### Create a new column using `apply()`\n","\n","Sometimes you need to create a new column based on more complex manipulation of existing data. In this example, we use the `apply()` method to apply the function `calc_hour` along the rows in the column `TIME`. The `calc_hour` function parses an integer value of the hour from a string containing the time at which a strike occurred. We create a new column `HOUR` that contains a numerical representation of the hour in which a strike occurred."]},{"cell_type":"code","metadata":{"id":"jdtZZcTdJAHg"},"source":["# Define a function that takes a time string in the form \"HH:MM\" and returns the\n","# hour as an integer if the hour value is valid\n","def calc_hour(time_str):\n","    # Split the string on the occurance of \":\" and take the first value\n","    hour = time_str.split(':')[0]\n","    # If the string is not empty return the strint cast as an integer\n","    if hour.strip(' ') != '':\n","        return int(hour)\n","\n","# Print out the \"TIME\" column\n","\n","\n","# Use the DataFrame apply() method to call calc_hour on the \"TIME\" column and\n","# create a new column \"HOUR\" in our DataFrame\n","\n","\n","# Print out the \"TIME\" and \"HOUR\" columns from our DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2RNfRsR13Mx"},"source":["#### Replace values in a column\n","\n","We can replace values in a column based on conditions, similar to \"find and replace.\" In this example, we make our new `SINGLE_OR_MULTI_ENGINE` column more descriptive by changing `True` into \" Single engine\" and `False` into \"Multi engine\".\n","\n"]},{"cell_type":"code","metadata":{"id":"TbS4YUYV17sE"},"source":["# Replace True or False values with new strings, \"Single engine\" or \"Multi engine\"\n","\n","\n","# Print out the updated column of data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KtowpcFjYB98"},"source":["#### Filtering\n","\n","We can filter our data using conditional statements. This can help us remove unecessary rows of data or observe a specific range of data."]},{"cell_type":"code","metadata":{"id":"u4lPRIIF9wKB"},"source":["# Print out unique values from column \"TIME_OF_DAY\"\n","\n","\n","# Filter the data to only see incidents that happened at night\n","\n","\n","# Print out the filtered data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrY9dUg-YEjI"},"source":["# Filter the data using \"INCIDENT_YEAR\" to only see data from 2010 and after\n","\n","\n","# Print out the filtered data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TfhcH7Hv9-Zx"},"source":["# Filter the data to only see incidents from 2010 and after that happened at\n","# night using the columns \"TIME_OF_DAY\" and \"INCIDENT_YEAR\"\n","\n","\n","# Print out the filtered data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHdCEIZkdhoa"},"source":["### Write data to a file\n","\n","After completing our manipulation, we may want to save our final dataset for future analyses."]},{"cell_type":"code","metadata":{"id":"SAC_xBWbeGuv"},"source":["# Write to csv\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUnZceXrWtpC"},"source":["## Collaborative exercises\n","\n","We can use this time to work on the following exercises or ask questions."]},{"cell_type":"markdown","metadata":{"id":"qEWRXGuubDfE"},"source":["### Exercise 1: Indexing cells\n","\n","Use referencing and indexing to answer the following questions by finding the data in the rows, columns, and/or cells. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"WfxKanr3aY6F"},"source":["#### 1a. Time of day\n","Airlines are interested in when they should schedule flights to minimize collisions. What is the time of day for each incident? Create a `Series` of the time of day (`TIME_OF_DAY`)"]},{"cell_type":"code","metadata":{"id":"9HrY_oNqckxz"},"source":["# Create a `Series` of the time of day (`TIME_OF_DAY`)\n","\n","\n","# Print new Series\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nlEFmkWgabWq"},"source":["#### 1b. Date and time\n","We want to find out when most of these collisions occur. What is the exact date and time of each incident? Print the third, fourth, and fifth columns from the data (`INCIDENT_MONTH`,\t`INCIDENT_YEAR`, and\t`TIME`)."]},{"cell_type":"code","metadata":{"id":"KBaZgZcSckqD"},"source":["# Print the third, fourth, and fifth columns from the data \n","# (`INCIDENT_MONTH`, `INCIDENT_YEAR`, and `TIME`).\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vn0xaQufdoWm"},"source":["#### 1c. Access the 126th row\n","\n","Use row indexing to find the data in the 126th row in the `wl_strikes_json` DataFrame. Check that your result is correct by making sure your `INCIDENT_DATE` value is `2020-07-17`.\n","\n","> Tip: Remember that the integer-based row location is zero based\n","\n"]},{"cell_type":"code","metadata":{"id":"q3Bs5MEAeAtn"},"source":["# Access the 126th row from the 'wl_strikes_json` DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ua1uIGpgad7i"},"source":["#### 1d. Cloud cover\n","A particular airline has nine flights that they want to compare to see if the cloud cover in the area had anything to do with the collision. Print rows 60-65 and the columns `INDX_NR`, `SKY`, `PHASE_OF_FLIGHT`, and `AIRPORT`"]},{"cell_type":"code","metadata":{"id":"z0Ysc1yuckhC"},"source":["# 2Print rows 60-65 and the columns 'INDX_NR', 'SKY', 'PHASE_OF_FLIGHT', and\n","# 'AIRPORT'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAqsOIs2w89S"},"source":["### Exercise 2: Rename column headers\n","\n","Rename the column `REG` to the more descriptive `AIRCRAFT_REGISTRATION`"]},{"cell_type":"code","metadata":{"id":"kPib5An85g_f"},"source":["# Change the column name \"REG\" to \"AIRCRAFT_REGISTRATION\"\n","\n","\n","# Print out the new DataFrame columns\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oh08mPHb-0RB"},"source":["### Exercise 3: Remove unnecessary columns\n","\n","The are several columns of data that are not relevant for our analyses. Remove all columns related to engine and damage location (e.g., all the columns that begin with `ENG_`, `DAM_`, and `STR_`). A list of these column names is provided in the variable `drop_columns`.\n","\n","**Bonus:** See if you can derive the column names in the list `drop_columns` from the dataset"]},{"cell_type":"code","metadata":{"id":"y0u9pN2ggWn7"},"source":["# A list of column names to remove from the DataFrame\n","drop_columns = ['ENG_1_POS', 'ENG_2_POS', 'ENG_3_POS', 'ENG_4_POS', 'STR_RAD',\n","                'DAM_RAD', 'STR_WINDSHLD', 'DAM_WINDSHLD', 'STR_NOSE',\n","                'DAM_NOSE', 'STR_ENG1', 'DAM_ENG1', 'STR_ENG2', 'DAM_ENG2',\n","                'STR_ENG3', 'DAM_ENG3', 'STR_ENG4', 'DAM_ENG4', 'STR_PROP',\n","                'DAM_PROP', 'STR_WING_ROT', 'DAM_WING_ROT', 'STR_FUSE',\n","                'DAM_FUSE', 'STR_LG', 'DAM_LG', 'STR_TAIL', 'DAM_TAIL',\n","                'STR_LGHTS', 'DAM_LGHTS', 'STR_OTHER', 'DAM_OTHER']\n","\n","# Remove unnecessary columns\n","\n","\n","# Print out the new DataFrame columns\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8TSrmwW868c"},"source":["### Exercise 4: Filter out unnecessary rows\n","\n","Our dataset should only contain data from the years 1990-2019. Remove any rows of data that contain strikes that occurred outside of this year range."]},{"cell_type":"code","metadata":{"id":"3HbTYcuL9Xgj"},"source":["# Filter out rows of data that contain strikes that occurred outside of the year\n","# range 1990-2019\n","\n","\n","# Print out the new filtered DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Hc8TS8Xar8O"},"source":["### Exercise 5: Join airline operator names with the full dataset\n","\n","Our dataset contains a column of airline operator IDs (`OPID`). These IDs correspond with airline operator names (e.g., Delta Airlines, Military, United Airlines, etc.). We have another dataset that contains arline operator IDs (in a column named `OPID`) and the corresponding airline operator name (in a column named `OPERATOR`). The URL to this dataset is stored in the variable `op_name_file_url`. Load this dataset and use a left join to merge the operator name with the full dataset."]},{"cell_type":"code","metadata":{"id":"iSQmchxx-BFU"},"source":["# URL to the CSV file containing unique airline operator IDs and names\n","op_name_file_url = 'https://github.com/NCSU-Libraries/data-viz-workshops/blob/master/Python_Open_Labs/data/FAA_Wildlife_operator_id_table.csv?raw=true'\n","\n","# Load the operator ID and name dataset into a DataFrame\n","\n","\n","# Join airline operator names to the full dataset using matching operater IDs\n","\n","\n","# Print out the columns \"OPID\" and \"OPERATOR\" from the new merged DataFrame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"etgjBVCQoGWv"},"source":["### Exercise 6: Create a new column containing month names\n","\n","Our dataset currently contains a column of integer values representing the month number in which a strike occurred (1-12). It would be helpful to have a column containing the month name (e.g., January, February, etc.). Calculate a new column labeled `MONTH_NAME` containing the month name in which a stike occurred.\n","\n","**TIP:** There are multiple ways you could consider creating this new column (e.g., using `replace()` or `apply()`), but it might be helpful to have a way to map month numbers (1-12) to month names (January - December) (e.g., a list or dictionary)."]},{"cell_type":"code","metadata":{"id":"i1ti8X6sYLvM"},"source":["# Create a new column \"MONTH_NAME\" containing month names in which a strike occurred\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_dY4wqNdRSD"},"source":["## Further resources\n","\n","### Filled version of this notebook\n","\n","[Data Cleaning in Python with pandas filled notebook](https://colab.research.google.com/github/ncsu-libraries-data-vis/data-cleaning-in-python-with-pandas/blob/main/Data_Cleaning_in_Python_with_pandas_filled.ipynb) - a version of this notebook with all code filled in for the guided activity and exercises.\n","\n","### Learning resources\n","\n","- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/index.html) - a free, online version of Jake VanderPlas' introduction to data science with Python, includes a chapter on data manipulation with pandas.\n","- [Python Programming for Data Science](https://www.tomasbeuzen.com/python-programming-for-data-science/README.html) - a website providing a great overview of conducting data science with Python including pandas.\n","\n","### Finding help with pandas\n","\n","The [Pandas website](https://pandas.pydata.org/) and [online documentation](http://pandas.pydata.org/pandas-docs/stable/) are useful resources, and of course the indispensible [Stack Overflow has a \"pandas\" tag](https://stackoverflow.com/questions/tagged/pandas).  There is also a (much younger, much smaller) [sister site dedicated to Data Science questions that has a \"pandas\" tag](https://datascience.stackexchange.com/questions/tagged/pandas) too."]},{"cell_type":"markdown","metadata":{"id":"i6Sj-7OYR5JL"},"source":["## Credits\n","\n","This workshop was created by Walt Gurley and Claire Cahoon, adapted from previous workshop materials by Scott Bailey and Simon Wiles, of Stanford Libraries."]}]}